from pathlib import Path
import argparse
import subprocess
import logging


ASS_BASIC = '''
[Script Info]
; Script generated by Aegisub 3.3.3
; http://www.aegisub.org/
Title: ASS File
Original Script: pyannote/speaker-diarization-3.1@Huggingface.co & Whisper
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
YCbCr Matrix: None

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,48,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,0,0,0,0,100,100,0,0,1,2,2,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
'''

logging.basicConfig(level=logging.INFO, format='\033[1;31m[%(asctime)s] %(message)s\033[0m')


def load_audio(media_path):
    logging.info('Loading {}'.format(media_path))

    from torch import frombuffer, int16, float32
    cmd = [
        'ffmpeg',
        '-nostdin',
        "-threads", "0",
        '-i', media_path,
        '-f', 's16le',
        '-ac', '1',
        '-acodec', 'pcm_s16le',
        '-ar', '16000',
        '-'
    ]

    try:
        out = subprocess.run(cmd, capture_output=True, check=True).stdout
    except subprocess.CalledProcessError as err:
        raise RuntimeError('Failed to load audio: {}'.format(err.stderr.decode('utf-8'))) from err

    audio_data = frombuffer(out, dtype=int16).to(float32) / 32768.0
    return audio_data


def asr(media_path, output_path, language, enable_gpu, whisper_model, segmentation_model):
    logging.info('Performing ASR operations')

    from torch import device
    from torch.cuda import is_available
    from pyannote.audio import Pipeline
    import whisper

    waveform = load_audio(media_path)

    logging.info('Loading segmentation model {}'.format(segmentation_model))
    pipeline = Pipeline.from_pretrained(segmentation_model)
    if enable_gpu and is_available():
        pipeline.to(device("cuda"))

    logging.info('Segmenting')
    segment_list = pipeline({
        "waveform": waveform.reshape(1, -1),
        "sample_rate": 16000
    })

    logging.info('Loading whisper model {}'.format(whisper_model))
    model = whisper.load_model(
        whisper_model,
        device=device('cuda') if enable_gpu and is_available() else None
    )

    logging.info('Transcribing')
    ass_text = ASS_BASIC
    for index, segment_info in enumerate(segment_list.itertracks(True)):
        segment, _, speaker_id = segment_info
        waveform_segment = waveform[int(segment.start * 16000):int(segment.end * 16000)]
        text = model.transcribe(waveform_segment, language=language)['text']
        line_text = 'Dialogue: 0,{start},{end},Default,{speaker},0,0,0,,{text}\n'.format(
            start=segment._str_helper(segment.start)[2:-1],
            end=segment._str_helper(segment.end)[2:-1],
            speaker=speaker_id,
            text=text
        )
        ass_text += line_text

    logging.info('Writing output to {}'.format(output_path))
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(ass_text)
        f.close()
    logging.info('Finished')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Automatically segment audio and perform ASR '
                                                 'operations then output an ASS file')

    parser.add_argument('-i', '--input', type=Path, help="path to a media file", required=True)
    parser.add_argument('-l', '--language', type=str, default='ja', help="language used to transcribe")
    parser.add_argument('-g', '--gpu', type=bool, default=True,
                        help="automatically enable GPU to process when it is supported")
    parser.add_argument('-w', '--whisper', type=str, default='medium',
                        help='name or path to whisper model')
    parser.add_argument('-s', '--segmentation', type=str, default='pyannote/speaker-diarization-3.1',
                        help='name or path to audio segmentation model (pyannote models are the only supportive now)')
    parser.add_argument('-o', '--output', type=Path, default=Path(r'.\asr.ass'), help='output ass file path')

    args = parser.parse_args()

    asr(args.input, args.output, args.language, args.gpu, args.whisper, args.segmentation)

    # Language = 'japanese'
    # EnableCUDA = True
    # MediaPath = r'./test/test.spleeter.cut.wav'
    # WhisperModel = r'./pretrained_models/whisper/base.pt'
    # PyannoteModel = r'./pretrained_models/pyannote/speaker-diarization-3.1/config.yaml'
    # OutputFilePath = r'./test/test.ass'
    # asr(MediaPath, OutputFilePath, Language, EnableCUDA, WhisperModel, PyannoteModel)
